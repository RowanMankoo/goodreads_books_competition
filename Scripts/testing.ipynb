{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "import torch\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from rowan_ds_tools.EDA import nlp\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "\n",
    "    def __init__(self,X,Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.n_samples = X.shape[0]\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        return self.X[index], self.Y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('..\\Data\\goodreads_train.csv')\n",
    "\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token='<UNK>', char_level=False)\n",
    "tokenizer.fit_on_texts(df['review_text'])\n",
    "model_input_dim = tokenizer.num_words\n",
    "\n",
    "# Tokenize and Pad sequences\n",
    "sequences = tokenizer.texts_to_sequences(df['review_text'])\n",
    "sequences = pad_sequences(sequences,maxlen=1500, padding='post')\n",
    "\n",
    "X = torch.tensor(sequences)\n",
    "Y = torch.tensor(df['rating'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take 90% of data for training and 10% for validation\n",
    "train_len = int(X.shape[0]*0.9 )\n",
    "\n",
    "X_train, X_val = X[:train_len,:], X[train_len:,:]\n",
    "Y_train, Y_val = Y[:train_len], Y[train_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 1500])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = TextDataset(X_train,Y_train)\n",
    "dataloader_train = DataLoader(dataset=dataset_train, batch_size=10,shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81000"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataloader_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_val = TextDataset(X_val,Y_val)\n",
    "dataloader_val = DataLoader(dataset=dataset_val, batch_size=10,shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class RatingModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, model_input_dim):\n",
    "\n",
    "        super(RatingModel,self).__init__()\n",
    "        self.layer1 = torch.nn.Embedding(model_input_dim, 150) # -> (*,H)\n",
    "        self.layer2 = torch.nn.GRU(150,100,num_layers=2,bidirectional=True) # -> (D*num_layers,N,H) D-2 if bidrectional, N-batch_size, H-hidden_size \n",
    "        self.layer3 = torch.nn.Linear(2*100,6)\n",
    "\n",
    "    \n",
    "    def forward(self,x):\n",
    "        \n",
    "        out = self.layer1(x)\n",
    "        out, hidden = self.layer2(out)\n",
    "        # Take the last output of RNN\n",
    "        out = out[:,-1,:]\n",
    "        out = self.layer3(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RatingModel(model_input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, criterion, optimiser, device='cpu', backpropogate=True):\n",
    "    epoch_loss = 0\n",
    "    for i, (X,Y) in enumerate(dataloader):\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        outputs = model(X)\n",
    "        loss = criterion(outputs,Y)\n",
    "\n",
    "        if backpropogate:\n",
    "            loss.backwards()\n",
    "            optimiser.step()\n",
    "        \n",
    "        # Get average epoch loss per observation\n",
    "        epoch_loss += loss.item()/(dataloader.batch_size*len(dataloader))  \n",
    "\n",
    "        return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "810000 81000\n",
      "[1,   200] loss: 1.512\n",
      "[1,   400] loss: 1.497\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [130], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m outputs \u001b[39m=\u001b[39m model(X)\n\u001b[0;32m     18\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs,Y)\n\u001b[1;32m---> 19\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     20\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     22\u001b[0m running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 2\n",
    "total_samples = len(dataset)\n",
    "n_iterations = math.ceil(total_samples/10)\n",
    "print(total_samples, n_iterations)\n",
    "\n",
    "training_loss = []\n",
    "val_loss = []\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    train_epoch_loss = train_one_epoch(model,dataloader_train, criterion, optimizer)\n",
    "    training_loss.append(train_epoch_loss)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_epoch_loss = train_one_epoch(model,dataloader_train, criterion, optimizer, backpropogate=False)\n",
    "    val_loss.append(val_epoch_loss)\n",
    "\n",
    "            \n",
    "print('Finsihed Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.8341, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 1,833,606 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c4af7d6242082e4131a702f983280a032ccd33ed19d564fcc781d886f19b72ff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
